{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afraid-throw",
   "metadata": {},
   "source": [
    "In this notebook we will use machine learning algorithms to predict mode choice for the SwissMetro dataset, using the `scikit-learn` library. We will compare the procedure and results with discrete choice models using the `BIOGEME` library. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f07108",
   "metadata": {},
   "source": [
    "# DCMs \n",
    "\n",
    "We can first establish a discrete choice model benchmark on the SwissMetro data using Biogeme. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5bcb45",
   "metadata": {},
   "source": [
    "Version of Biogeme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e00da806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biogeme 3.2.8 [2021-09-02]\n",
      "Version entirely written in Python\n",
      "Home page: http://biogeme.epfl.ch\n",
      "Submit questions to https://groups.google.com/d/forum/biogeme\n",
      "Michel Bierlaire, Transport and Mobility Laboratory, Ecole Polytechnique Fédérale de Lausanne (EPFL)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import biogeme.version as ver\n",
    "print(ver.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "272ccefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_root = (\n",
    "    'https://courses.edx.org/'\n",
    "    'asset-v1:EPFLx+ChoiceModels2x+3T2021+type@asset+block@'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-background",
   "metadata": {},
   "source": [
    "First we can import our libraries and setup the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "spare-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import biogeme.database as db\n",
    "import biogeme.biogeme as bio\n",
    "from biogeme import models\n",
    "from biogeme.expressions import Beta\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_table(f'{url_root}swissmetro.dat', sep='\\t')\n",
    "database = db.Database('swissmetro', df)\n",
    "\n",
    "# The following statement allows you to use the names of the variable\n",
    "# as Python variable.\n",
    "globals().update(database.variables)\n",
    "\n",
    "# Removing some observations\n",
    "exclude = ((PURPOSE != 1) * (PURPOSE != 3) + (CHOICE == 0)) > 0\n",
    "database.remove(exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-botswana",
   "metadata": {},
   "source": [
    "We can now specify our model. We will define a very simple model here, including only generic parameters for time and cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "threaded-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to be estimated\n",
    "ASC_CAR = Beta('ASC_CAR', 0, None, None, 0)\n",
    "ASC_TRAIN = Beta('ASC_TRAIN', 0, None, None, 0)\n",
    "ASC_SM = Beta('ASC_SM', 0, None, None, 1)\n",
    "B_TIME = Beta('B_TIME', 0, None, None, 0)\n",
    "B_COST = Beta('B_COST', 0, None, None, 0)\n",
    "\n",
    "\n",
    "# Definition of new variables\n",
    "SM_COST = SM_CO * (GA == 0)\n",
    "TRAIN_COST = TRAIN_CO * (GA == 0)\n",
    "CAR_AV_SP = CAR_AV * (SP != 0)\n",
    "TRAIN_AV_SP = TRAIN_AV * (SP != 0)\n",
    "TRAIN_TT_SCALED = TRAIN_TT / 100.0\n",
    "TRAIN_COST_SCALED = TRAIN_COST / 100\n",
    "SM_TT_SCALED = SM_TT / 100.0\n",
    "SM_COST_SCALED = SM_COST / 100\n",
    "CAR_TT_SCALED = CAR_TT / 100\n",
    "CAR_CO_SCALED = CAR_CO / 100\n",
    "\n",
    "# Definition of the utility functions\n",
    "V1 = ASC_TRAIN + B_TIME * TRAIN_TT_SCALED + B_COST * TRAIN_COST_SCALED\n",
    "V2 = ASC_SM + B_TIME * SM_TT_SCALED + B_COST * SM_COST_SCALED\n",
    "V3 = ASC_CAR + B_TIME * CAR_TT_SCALED + B_COST * CAR_CO_SCALED\n",
    "\n",
    "# Associate utility functions with the numbering of alternatives\n",
    "V = {1: V1, 2: V2, 3: V3}\n",
    "\n",
    "# Associate the availability conditions with the alternatives\n",
    "av = {1: TRAIN_AV_SP, 2: SM_AV, 3: CAR_AV_SP}\n",
    "\n",
    "# Definition of the model. This is the contribution of each\n",
    "# observation to the log likelihood function.\n",
    "logprob = models.loglogit(V, av, CHOICE)\n",
    "\n",
    "# Create the Biogeme object\n",
    "biogeme = bio.BIOGEME(database, logprob)\n",
    "biogeme.modelName = 'swiss_metro_validation'\n",
    "\n",
    "# Estimate the parameters\n",
    "results = biogeme.estimate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f752db0",
   "metadata": {},
   "source": [
    "As we are comparing with machine learning, we need to perform out of sample validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147b3a9",
   "metadata": {},
   "source": [
    "The `split` function below organizes the data into several slices (in this case 5) of about the same size, randomly defined. We make sure that groups of observations belong to the same data set.\n",
    "Each slice is considered as a validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e46a327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = 'ID'\n",
    "def split(slices):\n",
    "    ids = df[groups].unique()\n",
    "    np.random.shuffle(ids)\n",
    "    the_slices_ids = np.array_split(ids, slices)\n",
    "    theSlices = [\n",
    "        df[df[groups].isin(ids)]\n",
    "        for ids in the_slices_ids\n",
    "    ]\n",
    "    estimationSets = []\n",
    "    validationSets = []\n",
    "    for i, v in enumerate(theSlices):\n",
    "        estimationSets.append(\n",
    "            pd.concat(theSlices[:i] + theSlices[i + 1:])\n",
    "        )\n",
    "        validationSets.append(v)\n",
    "    return zip(estimationSets, validationSets)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4d97c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "validationData = split(slices=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-stress",
   "metadata": {},
   "source": [
    "The `validate` function then re-estimates the model using all the data except the slice, and the estimated model is applied on the validation set (i.e. the slice). \n",
    "The value of the log likelihood for each observation in the validation set is reported in a dataframe. \n",
    "As this is done for each slice, the output is a list of dataframes, each corresponding to one of these exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pursuant-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_results = biogeme.validate(results, validationData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-turkey",
   "metadata": {},
   "source": [
    "We can then output the normalized log-likelihood loss by calculating the mean of each of the log-likelihood values for each observation in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "surprising-birmingham",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalised log likelihood for 1359 validation data: -0.7209414127335014\n",
      "Normalised log likelihood for 1359 validation data: -0.8602697129740046\n",
      "Normalised log likelihood for 1350 validation data: -0.800106220549936\n",
      "Normalised log likelihood for 1350 validation data: -0.7671216275484689\n",
      "Normalised log likelihood for 1350 validation data: -0.7900951793680111\n"
     ]
    }
   ],
   "source": [
    "for slide in validation_results:\n",
    "    print(\n",
    "        f'Normalised log likelihood for {slide.shape[0]} validation data: '\n",
    "        f'{slide[\"Loglikelihood\"].mean()}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-eleven",
   "metadata": {},
   "source": [
    "# Machine learning\n",
    "\n",
    "We can now try to test alternative machine learning classifiers using `scikit-learn`. \n",
    "\n",
    "First we need to import our libraries. \n",
    "We will investigate the Logistic Regression and Random Forest classifiers.\n",
    "\n",
    "We will also need to scale our data using the standard scaler, and evaluate the models using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hawaiian-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-native",
   "metadata": {},
   "source": [
    "Now we can set up our data. \n",
    "\n",
    "The machine learning model will consider all features provided in the input dataset. \n",
    "\n",
    "We therefore need to remove the target that we wish to predict and the context columns that don't tell us anything about the choice situation. \n",
    "\n",
    "We store the target as `y`. \n",
    "\n",
    "We then scale our input data and store it as `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "solved-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_context = ['CHOICE', 'GROUP', 'SURVEY', 'SP', 'ID']\n",
    "y = df.CHOICE\n",
    "X_unscaled = df[[col \n",
    "                 for col in df.columns \n",
    "                 if col not in ['CHOICE']]]\n",
    "X = StandardScaler().fit_transform(X_unscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-dictionary",
   "metadata": {},
   "source": [
    "Now we are ready to investigate our machine learning models. \n",
    "\n",
    "We can first try the logistic regression classifier. \n",
    "\n",
    "It uses the same logistic function as the discrete choice model, though all variables are included uniformly for all choices, and so does not output utilities. \n",
    "\n",
    "We first define our classifier, specifying any parameters we may need. \n",
    "\n",
    "Here we use a smaller `C` parameter, increasing the strength of the `L2` regularization. \n",
    "\n",
    "We can then perform cross-validation in the same way we did for the discrete choice model, remembering to specify the scoring to use the normalised log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceramic-substance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.93692112, -1.00159854, -0.81403925, -0.72420042, -0.93098119])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C=0.001)\n",
    "lr_scores = cross_val_score(\n",
    "    lr, X, y, cv=5, groups=df.ID, scoring='neg_log_loss'\n",
    ")\n",
    "lr_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-custom",
   "metadata": {},
   "source": [
    "We could also repeat the procedure for a different model, for instance a Random Forest Classifier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "difficult-dakota",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.03909524, -1.07049627, -0.96614977, -0.77450147, -1.24844636])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100, max_depth=3\n",
    ")\n",
    "rf_scores = cross_val_score(\n",
    "    rf, X, y, cv=5, groups=df.ID, scoring='neg_log_loss'\n",
    ")\n",
    "rf_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-retailer",
   "metadata": {},
   "source": [
    "It seems that the machine learning models do not perform as well with these parameters as our very simple discrete choice model. \n",
    "\n",
    "This is because the SwissMetro dataset is very small, and there is not enough data for the models to generalize to. \n",
    "\n",
    "We can therefore see what happens when we use a larger dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
