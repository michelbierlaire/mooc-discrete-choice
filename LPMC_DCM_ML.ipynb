{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bibliographic-mumbai",
   "metadata": {},
   "source": [
    "We will now compare ML and DCM on the much larger and more complex London Passenger Mode Choice data. \n",
    "\n",
    "# DCMs\n",
    "\n",
    "We can now establish a DCM benchmark using the same steps as we did for the SwissMetro case study. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e52bc7a",
   "metadata": {},
   "source": [
    "[Click here for a description of the data](http://transp-or.epfl.ch/documents/technicalReports/CS_LPMC.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fc11d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_root = (\n",
    "    'https://courses.edx.org/'\n",
    "    'asset-v1:EPFLx+ChoiceModels2x+3T2021+type@asset+block@'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stopped-atlas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import biogeme.database as db\n",
    "import biogeme.biogeme as bio\n",
    "from biogeme import models\n",
    "from biogeme.expressions import Beta\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_table(f'{url_root}lpmc.dat', sep='\\t')\n",
    "database = db.Database('lpmc', df)\n",
    "\n",
    "# The following statement allows you to use the names of the variable\n",
    "# as Python variable.\n",
    "globals().update(database.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-morning",
   "metadata": {},
   "source": [
    "This time we will use a more complex model specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "brutal-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to be estimated\n",
    "ASC_WALKING = Beta('ASC_WALKING', 0, None, None, 1)\n",
    "ASC_CYCLING = Beta('ASC_CYCLING', 0, None, None, 0)\n",
    "ASC_PT = Beta('ASC_PT', 0, None, None, 0)\n",
    "ASC_DRIVING = Beta('ASC_DRIVING', 0, None, None, 0)\n",
    "B_TIME_WALKING = Beta('B_TIME_WALKING', 0, None, None, 0)\n",
    "B_TIME_CYCLING = Beta('B_TIME_CYCLING', 0, None, None, 0)\n",
    "B_TIME_DRIVING = Beta('B_TIME_DRIVING', 0, None, None, 0)\n",
    "B_COST_DRIVING = Beta('B_COST_DRIVING', 0, None, None, 0)\n",
    "B_COST_PT = Beta('B_COST_PT', 0, None, None, 0)\n",
    "B_TIME_PT_BUS = Beta('B_TIME_PT_BUS', 0, None, None, 0)\n",
    "B_TIME_PT_RAIL = Beta('B_TIME_PT_RAIL', 0, None, None, 0)\n",
    "B_TIME_PT_ACCESS = Beta('B_TIME_PT_ACCESS', 0, None, None, 0)\n",
    "B_TIME_PT_INT = Beta('B_TIME_PT_INT_WAIT', 0, None, None, 0)\n",
    "B_TRAFFIC_DRIVING = Beta('B_TRAFFIC_DRIVING', 0, None, None, 0)\n",
    "\n",
    "# Utility functions\n",
    "\n",
    "V1 = (\n",
    "    ASC_WALKING + \n",
    "    B_TIME_WALKING * dur_walking\n",
    ")\n",
    "\n",
    "V2 = (\n",
    "    ASC_CYCLING +\n",
    "    B_TIME_CYCLING * dur_cycling\n",
    ")\n",
    "\n",
    "V3 = (\n",
    "    ASC_PT +\n",
    "    B_COST_PT * cost_transit + \n",
    "    B_TIME_PT_ACCESS * dur_pt_access + \n",
    "    B_TIME_PT_RAIL * dur_pt_rail + \n",
    "    B_TIME_PT_BUS * dur_pt_bus +\n",
    "    B_TIME_PT_INT * dur_pt_int\n",
    ")\n",
    "      \n",
    "V4 = (\n",
    "    ASC_DRIVING +\n",
    "    B_TIME_DRIVING * dur_driving +\n",
    "    B_COST_DRIVING * (cost_driving_fuel + cost_driving_ccharge) +\n",
    "    B_TRAFFIC_DRIVING * driving_traffic_percent\n",
    ")\n",
    "      \n",
    "# Associate utility functions with the numbering of alternatives\n",
    "V = {1: V1,\n",
    "     2: V2,\n",
    "     3: V3,\n",
    "     4: V4}\n",
    "\n",
    "# Associate the availability conditions with the alternatives\n",
    "\n",
    "av = {1: 1,\n",
    "      2: 1,\n",
    "      3: 1,\n",
    "      4: 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "691cdbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the model. This is the contribution of each\n",
    "# observation to the log likelihood function.\n",
    "logprob = models.loglogit(V, av, travel_mode)\n",
    "\n",
    "# Create the Biogeme object\n",
    "biogeme = bio.BIOGEME(database, logprob)\n",
    "biogeme.modelName = 'lpmc_validation'\n",
    "\n",
    "# Estimate the parameters\n",
    "results = biogeme.estimate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00f7f96",
   "metadata": {},
   "source": [
    "The validation consists in organizing the data into several slices\n",
    "of about the same size, randomly defined. Each slice is considered\n",
    "as a validation dataset. The model is then re-estimated using all\n",
    "the data except the slice, and the estimated model is applied on the\n",
    "validation set (i.e. the slice). The value of the log likelihood for\n",
    "each observation in the validation set is reported in a\n",
    "dataframe. As this is done for each slice, the output is a list of\n",
    "dataframes, each corresponding to one of these exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1382acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = 'household_id'\n",
    "def split(slices):\n",
    "    ids = df[groups].unique()\n",
    "    np.random.shuffle(ids)\n",
    "    the_slices_ids = np.array_split(ids, slices)\n",
    "    theSlices = [\n",
    "        df[df[groups].isin(ids)]\n",
    "        for ids in the_slices_ids\n",
    "    ]\n",
    "    estimationSets = []\n",
    "    validationSets = []\n",
    "    for i, v in enumerate(theSlices):\n",
    "        estimationSets.append(\n",
    "            pd.concat(theSlices[:i] + theSlices[i + 1:])\n",
    "        )\n",
    "        validationSets.append(v)\n",
    "    return zip(estimationSets, validationSets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "guided-backing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood for 16511 validation data: -0.8467425977830267\n",
      "Log likelihood for 16425 validation data: -0.8330816228984251\n",
      "Log likelihood for 16050 validation data: -0.8310820577984016\n",
      "Log likelihood for 16072 validation data: -0.8397447330034897\n",
      "Log likelihood for 16028 validation data: -0.8379201327242661\n"
     ]
    }
   ],
   "source": [
    "validationData = split(slices=5)\n",
    "\n",
    "validation_results = biogeme.validate(results, validationData)\n",
    "\n",
    "for slide in validation_results:\n",
    "    print(\n",
    "        f'Log likelihood for {slide.shape[0]} validation data: '\n",
    "        f'{slide[\"Loglikelihood\"].mean()}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-illness",
   "metadata": {},
   "source": [
    "# Machine learning\n",
    "\n",
    "Again we can use `scikit-learn` to investigate ML classifiers. \n",
    "\n",
    "The way we set up and use our models is exactly the same. \n",
    "\n",
    "However, as our data is panel data, we need to perform grouped cross validation, where the folds are grouped by household. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "frank-inventory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.69158506, -0.68885028, -0.68355673, -0.70359612, -0.70837667])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "target_context = ['travel_mode', 'trip_id', 'household_id', 'person_n', 'trip_n']\n",
    "y = df.travel_mode\n",
    "X = StandardScaler().fit_transform(df[[col for col in df.columns if col not in target_context]])\n",
    "\n",
    "lr = LogisticRegression(C=0.1, max_iter=1000)\n",
    "lr_scores = cross_val_score(\n",
    "    lr, X, y, \n",
    "    cv=GroupKFold(n_splits=5), \n",
    "    groups=df.household_id, \n",
    "    scoring='neg_log_loss')\n",
    "lr_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "alive-glance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.71498655, -0.71687217, -0.71371533, -0.72852786, -0.73048802])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=6)\n",
    "rf_scores = cross_val_score(\n",
    "    rf, X, y, \n",
    "    cv=GroupKFold(n_splits=5), \n",
    "    groups=df.household_id, \n",
    "    scoring='neg_log_loss')\n",
    "rf_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-burke",
   "metadata": {},
   "source": [
    "On the larger more complex dataset, the machine learning models appear to outperform the MNL model in terms of out of sample validation performance. \n",
    "\n",
    "However, there is a lot more we could explore, including different model specifications for the DCM, as well as different ML algorithms and parameter values!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
